# Papers, tools and data that used in Generating Deepfakes

For the convenience of reading and searching, the following is a list of papers about Deepfakes generation. The content might  be updated from time to time.

***Content***
- Korshunova et al. **[Fast Face-swap Using Convolutional Neural Networks](http://openaccess.thecvf.com/content_iccv_2017/html/Korshunova_Fast_Face-Swap_Using_ICCV_2017_paper.html)**
- Razavi et al. **[Generating Diverse High-Fidelity Images with VQ-VAE-2](https://arxiv.org/abs/1906.00446)**
- Turkoglu et al. **[A Layer-Based Sequential Framework for Scene Generation with GANs](https://arxiv.org/abs/1902.00671)**
  - Code:(https://github.com/0zgur0/Seq_Scene_Gen)
- Denton et al. **[Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks](https://arxiv.org/abs/1506.05751)**
- Gatys et al. **[Image style transfer using convolutional neural networks](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf)**

***In this section, we made some summary of these papers:***
- Korshunova et al. **[Fast Face-swap Using Convolutional Neural Networks](http://openaccess.thecvf.com/content_iccv_2017/html/Korshunova_Fast_Face-Swap_Using_ICCV_2017_paper.html)**: This paper comes up with a solution to face swapping problem in terms of style transfer using CNN.The proposed system in this paper has two additional components performing face alignment and background/hair/skin segmentation.  Facial keypoints were extracted using dlib, which are used to align a frontal view reference face.  Segmentation is used to re- store the background and hair of the input image X based on the cloning technique in OpenCV.Its transformation network here is a multiscale architecture with branches operating on different downsampled versions of the input image X, which is based on the architecture of Ulyanov et al.  Each such branch has blocks of zero-padded convo- lutions followed by linear rectification. Branches are combined via nearest-neighbor upsampling by a factor of two and concatenation along the channel axis. The last branch of the network ends with a 1 Ã— 1 convolution and 3 color channels. For every input image X, it aims to generate an X which jointly minimizes the following content and style loss. These losses are defined in the feature space of the normalised version of the 19-layer VGG network. Besides, this paper applies light loss to solve the problem that the lighting conditions of the content image x are not preserved in the generated image.

- Razavi et al. **[Generating Diverse High-Fidelity Images with VQ-VAE-2](https://arxiv.org/abs/1906.00446)**: This paper is based on Vector Quantized Variational AutoEncoder and succeed to scale and enhance its performance. The proposed models combines VQ-VAE and PixelCNN, of which the proposed method follows a two-stage approach: first, it trains a hierarchical VQ-VAE to encode images onto a discrete latent space, and then it fits a powerful PixelCNN prior over the discrete latent space induced by all the data. As opposed to vanilla VQ-VAE, in this work it uses a hierarchy of vector quantized codes to model large images. The main motivation behind this is to model local information, such as texture, separately from global information such as shape and geometry of objects. The prior model over each level can thus be tailored to capture the specific correlations that exist in that level. In order to further compress the image, and to be able to sample from the model learned during stage 1, it learns a prior over the latent codes. Fitting prior distributions using neural networks from training data has become common practice, as it can significantly improve the performance of latent variable models. The fidelity of  best class conditional samples are competitive with the state of the art Generative Adversarial Networks, with broader diversity in several classes, contrasting our method against the known limitations of GANs. 

- Turkoglu et al. **[A Layer-Based Sequential Framework for Scene Generation with GANs](https://arxiv.org/abs/1902.00671)**: The paper tackles the scene composition task with a layered structure coding approach, which helps reduce the complexity of the problem with one subproblem at a time. The idea is based on process of landscape painting from overall structure, e.g. mountain ranges, to other individual detailed objects, e..g. animals or trees. The main objective is to compose a realistic scene with allowing element-level control. The generator model is broken into two simpler subtasks. The first one is a background generator. Given a semantic layout map, the input takes a noise vector z_0 and generates a background image x_0. Loss function is defined similar to typical GAN architecture, but only pixels outside of the semantic layout mask are penalized.  
The second step is to add foreground objects. It takes as input previously generated scene x_(t-1), the current foreground object mask M_t and a noise vector z_t. Conditioned on the previous scene without the regions M_t ignoring, this task is turned into an image pinpointing problem. Loss function takes both local reconstruction loss, i.e. object region and global reconstruction, i.e. whole image. 

- Denton et al. **[Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks](https://arxiv.org/abs/1506.05751)**: The papers proposes an approach to break generation problem into a sequence of more manageable stages. At each scale, a CNN-based generative model is implemented. Samples are drawn in a coarse-to-fine fashion, starting with a low-frequency residual image. The subsequent levels sample the band-pass structure at the next level, conditioned on the output from previous scale, until final level. Laplacian pyramid is implemented within the architecture, which is a linear invertible image representation consisting of a set of band-pass images, plus a low-frequency residual. The paper names its network construction as Laplacian Generative Adversarial Networks (LAPGAN). It combines conditional GAN model with Laplacian pyramid representation.

- Gatys et al. **[Image style transfer using convolutional neural networks](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf)**: Rendering the semantic content of an image in different styles is a difficult image processing task. Arguably, a major limiting factor for previous approaches has been the lack of image representations that explicitly represent semantic information and, thus, allow to separate image content from style. Here this paper uses image representations derived from Convolutional Neural Networks optimized for object recognition, which make high level image information explicit. This paper introduces A Neural Algorithm of Artistic Style that can separate and recombine the image content and style of natural images. The algorithm allows us to produce new images of high perceptual quality that combine the content of an arbitrary photograph with the appearance of numerous well-known artworks. The results provide new insights into the deep image representations learned by Convolutional Neural Networks and demonstrate their potential for high level image synthesis and manipulation.
